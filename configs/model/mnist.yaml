_target_: lightning_torch.core.model_base.MyLightningModule
net:
  _target_: lightning_torch.torch_nn.simple_dense_net.SimpleDenseNet
  input_size: 784
  lin1_size: 256
  lin2_size: 256
  lin3_size: 256
  output_size: 10
lr: 0.001
weight_decay: 0.0005

# TODO extend to other hyperparams
#optimizer:
#  #  Adam-oriented deep learning
#  _target_: torch.optim.Adam
#  #  These are all default parameters for the Adam optimizer
#  lr: 0.001
#  betas: [ 0.9, 0.999 ]
#  eps: 1e-08
#  weight_decay: 0
#
#lr_scheduler:
#  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#  T_0: 10
#  T_mult: 2
#  eta_min: 0 # min value for the lr
#  last_epoch: -1
#  verbose: False


