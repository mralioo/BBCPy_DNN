{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633ddd76-e1b3-4084-922f-f5a34ca71920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import bbcpy\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.data.srm_datamodule import SRMDatamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55225346-4a3d-4cf6-8580-ce74ceb6889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import bbcpy.functions.helpers as helpers\n",
    "# from bbcpy.functions import ImportFunc\n",
    "from bbcpy.datatypes.eeg import Data\n",
    "from bbcpy.functions.artireject import averagevariance\n",
    "\n",
    "from bbcpy.datatypes.srm_eeg import SRM_Data\n",
    "\n",
    "\n",
    "class _GEVDsf(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    d = None\n",
    "    W = None\n",
    "    A = None\n",
    "\n",
    "    def calcAB(self, x, y=None):\n",
    "        return None, None\n",
    "\n",
    "    def calcPatterns(self, x):  #\n",
    "        \"\"\"\n",
    "        calculates spatial pattern for data x\n",
    "        :param x: data\n",
    "        :return: A spatial pattern\n",
    "        \"\"\"\n",
    "        covX = x.cov(target='all', estimator=self.estimator)\n",
    "        return covX @ self.W_allcmps @ np.linalg.pinv(self.W_allcmps.T @ covX @ self.W_allcmps)[:, self.selected_cmps]\n",
    "\n",
    "    def scoring(self, d, Y):  # just use EVs\n",
    "        return d\n",
    "\n",
    "    def select(self, score, n_cmps):  # this is the general case, CSP is a special case.\n",
    "        return np.flipud(np.argsort(score))[:n_cmps]\n",
    "\n",
    "    def fit(self, x, y=None, n_cmps=None):\n",
    "        '''Fit CSP'''\n",
    "        if n_cmps is None:\n",
    "            n_cmps = self.n_cmps\n",
    "        if y is not None:\n",
    "            x.y = y\n",
    "        A, B = self.calcAB(x, y)\n",
    "        d, W = sp.linalg.eigh(A, B)\n",
    "        score = self.scoring(d, W.T @ x)\n",
    "        if n_cmps == 'all':\n",
    "            selected_cmps = np.arange(x.nCh)\n",
    "        else:\n",
    "            selected_cmps = self.select(score, n_cmps)\n",
    "        self.d = d[selected_cmps]\n",
    "        self.W = W[:, selected_cmps]\n",
    "        self.selected_cmps = selected_cmps\n",
    "        self.W_allcmps = W\n",
    "        self.A = self.calcPatterns(x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        \"\"\"Apply CSP.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray, shape (n_matrices, n_channels, n_times)\n",
    "            Multi-channel time-series\n",
    "        y : ndarray, shape (n_trials, 2)\n",
    "            Marker positions and marker class.\n",
    "        Returns\n",
    "        -------\n",
    "        out : ndarray, shape (n_matrices, n_csp, n_time)\n",
    "            transformed\n",
    "        \"\"\"\n",
    "        return self.W.T @ x\n",
    "\n",
    "\n",
    "class PCA(_GEVDsf):\n",
    "    def __init__(self, n_cmps='all', excllev=None, estimator='scm', scoring=helpers.evscoring_EV,\n",
    "                 select=helpers.evselect_best):\n",
    "        self.n_cmps = n_cmps\n",
    "        self.excllev = excllev\n",
    "        self.estimator = estimator\n",
    "        self.scoring = scoring\n",
    "        self.select = select\n",
    "\n",
    "    def calcAB(self, x, y=None):\n",
    "        if isinstance(x, Data):\n",
    "            if self.excllev is not None:\n",
    "                Sigma_trial = x.cov(\n",
    "                    target='all')  # no estimator here because on single trials, this will fuck up excllev\n",
    "                covtr = np.trace(np.linalg.pinv(x.cov(target='all', estimator=self.estimator)) @ Sigma_trial, axis1=1,\n",
    "                                 axis2=2) / x.shape[1]\n",
    "                sel_tr = covtr <= self.excllev\n",
    "                covs = x[sel_tr].cov(target='all', estimator=self.estimator)\n",
    "            else:\n",
    "                covs = x.cov(target='all', estimator=self.estimator)\n",
    "            return covs, None\n",
    "        else:\n",
    "            c1 = sk.covariance.OAS().fit(x).covariance_\n",
    "            return c1, None\n",
    "\n",
    "\n",
    "class CSP(_GEVDsf):\n",
    "\n",
    "    def __init__(self, n_cmps=6, excllev=None, estimator='scm', scoring=helpers.evscoring_EV,\n",
    "                 select=helpers.evselect_best_csp):\n",
    "        self.n_cmps = n_cmps\n",
    "        self.excllev = excllev\n",
    "        self.estimator = estimator\n",
    "        self.scoring = scoring\n",
    "        self.select = select\n",
    "\n",
    "    def calcAB(self, x, y=None):\n",
    "        if isinstance(x, Data) or isinstance(x, SRM_Data):  # has method cov etc\n",
    "            if self.excllev is not None:\n",
    "                covs = averagevariance(x, self.excllev, self.estimator).cov(target='class', estimator=self.estimator)\n",
    "            else:\n",
    "                covs = x.cov(target='class', estimator=self.estimator)\n",
    "            return covs[0], covs[0] + covs[1]\n",
    "        else:  # I would delete the following but merged it for Gabriel. Do we want to write all functions also to work\n",
    "            # with arbitrary arrays? I think we should consider only our own datatypes.\n",
    "            classes = np.unique(y)\n",
    "            c1 = sk.covariance.OAS().fit(x[y == classes[0]]).covariance_\n",
    "            c2 = sk.covariance.OAS().fit(x[y == classes[1]]).covariance_\n",
    "            return c1, c1 + c2\n",
    "        # if self.excllev is not None:\n",
    "        #     covs = averagevariance(x, self.excllev, self.estimator).cov(target='class', estimator=self.estimator)\n",
    "        # else:\n",
    "        #     covs = x.cov(target='class', estimator=self.estimator)\n",
    "        # return covs[0], covs[0] + covs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b24237-9227-4c88-832d-2c0f327301d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Collecting subject S1 sessions from:  ../data/SMR/raw/\n",
      "INFO:root:Loading subject: S1 finalized (1 from 1)\n",
      "INFO:root:Prepare to Load : ['Session_1'] sessions\n",
      "INFO:root:Preprocessing data..\n",
      "INFO:root:Session_1 loaded; has the shape: (155, 62, 600)\n",
      "INFO:root:Loading sessions: Session_1 finalized (1 from 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/SMR/raw/\"\n",
    "ival= \"2s:8s:10ms\"\n",
    "bands= [8,13]\n",
    "chans= [\"*\"]\n",
    "classes= [\"R\", \"L\"]\n",
    "test_subjects_sessions_dict= {\"S1\": [1]}\n",
    "train_subjects_sessions_dict= {\"S1\": [1]}\n",
    "vali_subjects_sessions_dict={\"S1\": [1]}\n",
    "concatenate_subjects= True\n",
    "train_val_split=None\n",
    "\n",
    "srm_datamodule = SRMDatamodule(data_dir=data_dir,\n",
    "                                            ival=ival,\n",
    "                                            bands=bands,\n",
    "                                            chans=chans,\n",
    "                                            classes=classes,\n",
    "                                            test_subjects_sessions_dict=test_subjects_sessions_dict,\n",
    "                                            train_subjects_sessions_dict=train_subjects_sessions_dict,\n",
    "                                            vali_subjects_sessions_dict=vali_subjects_sessions_dict,\n",
    "                                            concatenate_subjects=concatenate_subjects,\n",
    "                                            train_val_split=train_val_split)\n",
    "\n",
    "data_train = srm_datamodule.load_data(train_subjects_sessions_dict,\n",
    "                                           concatenate_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a5d6ad-d9e8-4873-9329-85edb1c6b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train\n",
    "y_train = data_train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb7b2fb-7c9e-4bd1-a2fe-e04259c1cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distributed.scheduler:State start\n",
      "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:57210\n",
      "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:57215'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:57213'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:57214'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:57216'\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:57229', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:57229\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57232\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:57230', name: 1, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:57230\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57236\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:57234', name: 3, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:57234\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57240\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:57233', name: 2, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:57233\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57238\n",
      "INFO:distributed.scheduler:Receive client connection: Client-08c683b8-267f-11ee-8308-00216a39830c\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57241\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6d294c-1024-46a0-a4ef-f12a33e8e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "# Convert your data to Dask arrays or dataframes\n",
    "X_train_dask = da.from_array(X_train, chunks='auto')\n",
    "y_train_dask = da.from_array(y_train, chunks='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa1e92-07c4-4f4f-8c0a-36a3870c116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dask.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e306b5a-1c83-4dfb-a4de-428bc4ddfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45557b8f-10be-45b1-ab59-5d92442ea498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from bbcpy.pipeline import make_pipeline\n",
    "import bbcpy.functions.helpers as helpers\n",
    "from bbcpy.functions.base import ImportFunc\n",
    "from bbcpy.functions.spatial import CSP, MBCSP\n",
    "from bbcpy.functions.artireject import AverageVariance\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from dask_ml.decomposition import PCA\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.preprocessing import RobustScaler\n",
    "from dask_ml.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b49dada-e9fe-4000-bb74-a32b11bb67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold()\n",
    "var = ImportFunc(np.var, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4715684e-9d07-4371-914e-21d4ad9f6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c184-2ef2-4266-b9cf-b5c1262ee147",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train_dask\n",
    "y = y_train_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d522-5fb5-4262-be44-1e23bc7f81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "x = da.ones((1000,1000), chunks=(100,100))\n",
    "z = x.sum() # This uses Dask default local cluster\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b93831-e869-417b-954a-ed2fac9f2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "# Fit the model using Dask\n",
    "with joblib.parallel_backend('dask'):\n",
    "    print(cross_val_score(csp_pipeline, X_train_dask, y_train_dask, cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862f0b24-482d-4706-bd2b-cd7d3240adc0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distributed.scheduler:Receive client connection: Client-worker-100685a7-267f-11ee-84b8-00216a39830c\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:57253\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6B60>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6D70>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6F20>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C70D0>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C7280>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mparallel_backend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# print(cross_val_score(csp_pipeline, X_train_dask, y_train_dask, cv=cv))\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsp_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6B60>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6D70>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C6F20>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C70D0>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 340, in _fit\n    self._validate_steps()\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 231, in _validate_steps\n    raise TypeError(\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bbcpy.functions.base.ImportFunc object at 0x00000246249C7280>' (type <class 'bbcpy.functions.base.ImportFunc'>) doesn't\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "\n",
    "\n",
    "csp_pipeline = make_pipeline(CSP(scoring=helpers.evscoring_medvar,select=helpers.evselect_directorscut),\n",
    "                             var,\n",
    "                             np.log,\n",
    "                             LDA())\n",
    "cv = KFold()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    # print(cross_val_score(csp_pipeline, X_train_dask, y_train_dask, cv=cv))\n",
    "    print(cross_val_score(csp_pipeline, data_train, data_train.y, cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3261068a-568c-4e8e-b66b-4e68137d4019",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:121: FutureWarning: if_delegate_has_method was deprecated in version 1.1 and will be removed in version 1.3. Use available_if instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 543, in _fit_transform_one\n    Xt, yt = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\base.py\", line 851, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 43, in fit\n    A, B = self.calcAB(x, y)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 120, in calcAB\n    c1 = sk.covariance.OAS().fit(x[y == classes[0]]).covariance_\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\core.py\", line 1993, in __getitem__\n    dsk, chunks = slice_array(out, self.name, self.chunks, index2, self.itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 176, in slice_array\n    dsk_out, bd_out = slice_with_newaxes(out_name, in_name, blockdims, index, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 198, in slice_with_newaxes\n    dsk, blockdims2 = slice_wrap_lists(out_name, in_name, blockdims, index2, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 254, in slice_wrap_lists\n    return slice_slices_and_integers(out_name, in_name, blockdims, index)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 303, in slice_slices_and_integers\n    raise ValueError(\nValueError: Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 179, in fit\n    Xt, yt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 146, in _fit\n    X, y, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 545, in _fit_transform_one\n    Xt = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\base.py\", line 851, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 43, in fit\n    A, B = self.calcAB(x, y)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 120, in calcAB\n    c1 = sk.covariance.OAS().fit(x[y == classes[0]]).covariance_\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\core.py\", line 1993, in __getitem__\n    dsk, chunks = slice_array(out, self.name, self.chunks, index2, self.itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 176, in slice_array\n    dsk_out, bd_out = slice_with_newaxes(out_name, in_name, blockdims, index, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 198, in slice_with_newaxes\n    dsk, blockdims2 = slice_wrap_lists(out_name, in_name, blockdims, index2, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 254, in slice_wrap_lists\n    return slice_slices_and_integers(out_name, in_name, blockdims, index)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 303, in slice_slices_and_integers\n    raise ValueError(\nValueError: Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsp_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_dask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_dask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32m~\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 543, in _fit_transform_one\n    Xt, yt = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\base.py\", line 851, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 43, in fit\n    A, B = self.calcAB(x, y)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 120, in calcAB\n    c1 = sk.covariance.OAS().fit(x[y == classes[0]]).covariance_\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\core.py\", line 1993, in __getitem__\n    dsk, chunks = slice_array(out, self.name, self.chunks, index2, self.itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 176, in slice_array\n    dsk_out, bd_out = slice_with_newaxes(out_name, in_name, blockdims, index, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 198, in slice_with_newaxes\n    dsk, blockdims2 = slice_wrap_lists(out_name, in_name, blockdims, index2, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 254, in slice_wrap_lists\n    return slice_slices_and_integers(out_name, in_name, blockdims, index)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 303, in slice_slices_and_integers\n    raise ValueError(\nValueError: Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 179, in fit\n    Xt, yt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 146, in _fit\n    X, y, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\pipeline\\pipeline.py\", line 545, in _fit_transform_one\n    Xt = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\sklearn\\base.py\", line 851, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 43, in fit\n    A, B = self.calcAB(x, y)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\bbcpy\\functions\\spatial.py\", line 120, in calcAB\n    c1 = sk.covariance.OAS().fit(x[y == classes[0]]).covariance_\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\core.py\", line 1993, in __getitem__\n    dsk, chunks = slice_array(out, self.name, self.chunks, index2, self.itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 176, in slice_array\n    dsk_out, bd_out = slice_with_newaxes(out_name, in_name, blockdims, index, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 198, in slice_with_newaxes\n    dsk, blockdims2 = slice_wrap_lists(out_name, in_name, blockdims, index2, itemsize)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 254, in slice_wrap_lists\n    return slice_slices_and_integers(out_name, in_name, blockdims, index)\n  File \"C:\\Users\\alioo\\Desktop\\MA\\bbcpy_AutoML\\bbcpy_env\\lib\\site-packages\\dask\\array\\slicing.py\", line 303, in slice_slices_and_integers\n    raise ValueError(\nValueError: Arrays chunk sizes are unknown: (nan,)\n\nA possible solution: https://docs.dask.org/en/latest/array-chunks.html#unknown-chunks\nSummary: to compute chunks sizes, use\n\n   x.compute_chunk_sizes()  # for Dask Array `x`\n   ddf.to_dask_array(lengths=True)  # for Dask DataFrame `ddf`\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(csp_pipeline, X_train_dask, y_train_dask, cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e7f7a-383c-4597-b10f-72c33dde1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('dask'):\n",
    "    sklearn.model_selection.cross_validate(csp_pipeline, x, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d208a2-5ede-4c0f-a214-641679ff7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e5539-92af-45ae-967b-028616a581ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8798c-0107-4fea-84ce-54a4bc135af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import sklearn\n",
    "\n",
    "# Create your Scikit-learn estimator (e.g., RandomForestClassifier, SVC, etc.)\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# Wrap the estimator with Dask's ParallelPostFit\n",
    "dask_estimator = ParallelPostFit(estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbcpy_env",
   "language": "python",
   "name": "bbcpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
