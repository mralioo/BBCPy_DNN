# @package _global_

# to execute this experiment run:
# python train.py experiment=example
defaults:
  - override /data: smr_datamodule.yaml
  - override /model: eegconformer.yaml
  - override /trainer: gpu.yaml
  - override /paths: default.yaml
  - override /extras: default.yaml
  - override /hydra: default.yaml
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["transformer-v1","Within-Subject", "RL"]

seed: 12345

optimized_metric: "val/acc_best"

data:
  task_name: "LR"
  trial_type: "forced"
  ival: 2s:10s:1ms
  chans: [ "*" ]
  bands: [ 8, 20 ]
  loading_data_mode: "within_subject"
  process_noisy_channels: True
  ignore_noisy_sessions: False
  train_val_split: True
  cross_validation: False
  transform: False
  normalize: { norm_type: "std", norm_axis: 0 }
  batch_size: 32
  num_workers: 4
  pin_memory: True

model:
  criterion:
    _target_: torch.nn.BCEWithLogitsLoss
    _partial_: true
  plots_settings:
    plot_every_n_epoch: 10
  net:
    n_outputs: 2  # (int) – Number of outputs of the model. This is the number of classes in the case of classification.
    n_chans: 62   # (int) – Number of EEG channels.
    n_times: 8000 # (int) – Number of time samples of the input window.
    n_filters_time: 40 # (int) – Number of temporal filters, defines also embedding size.
    filter_time_length: 25  # (int) – Length of the temporal filter.
    pool_time_length: 75 # (int) – Length of temporal pooling filter.
    pool_time_stride: 15 # (int) – Length of stride between temporal pooling filters.
    drop_prob: 0.5 #(float) – Dropout rate of the convolutional layer.
    att_depth: 3 #(int) – Number of self-attention layers.
    att_heads: 10 #(int) – Number of attention heads.
    att_drop_prob: 0.5 #(float) – Dropout rate of the self-attention layer.
    final_fc_length: auto #(int | str) – The dimension of the fully connected layer.
    return_features: False #(bool) – If True, the forward method returns the features before the last classification layer. Defaults to False.
    add_log_softmax: False #(bool)

trainer:
  min_epochs: 10
  max_epochs: 50
  check_val_every_n_epoch: 1

paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: /input-data
  log_dir: ${oc.env:PROJECT_ROOT}/logs/11_11
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
  results_dir: ${paths.log_dir}/results # final results directory


logger:
  mlflow:
    experiment_name: Transformer-v1
    run_name: S1


