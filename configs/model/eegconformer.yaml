_target_: src.trainer.dnn_trainer.DnnLitModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

criterion:
  _target_: torch.nn.CrossEntropyLoss
  _partial_: true

plots_settings:
  # plots each n epochs
  plot_every_n_epoch: 10

net:
  _target_: src.models.components.EEG-Conformer.EEGConformer
  n_outputs: 2  # (int) – Number of outputs of the model. This is the number of classes in the case of classification.
  n_chans: 62   # (int) – Number of EEG channels.
  n_filters_time: 32 # (int) – Number of temporal filters, defines also embedding size.
  filter_time_length: 25  # (int) – Length of the temporal filter.
  pool_time_length: 200 # (int) – Length of temporal pooling filter.
  pool_time_stride: 20 # (int) – Length of stride between temporal pooling filters.
  drop_prob: 0.5 #(float) – Dropout rate of the convolutional layer.
  att_depth: 8 #(int) – Number of self-attention layers.
  att_heads: 3 #(int) – Number of attention heads.
  att_drop_prob: 0.5 #(float) – Dropout rate of the self-attention layer.
  final_fc_length: auto #(int | str) – The dimension of the fully connected layer.
  return_features: False #(bool) – If True, the forward method returns the features before the last classification layer. Defaults to False.
  n_times: 8000 # (int) – Number of time samples of the input window.
  add_log_softmax: False #(bool)