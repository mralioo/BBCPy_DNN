# @package _global_

# to execute this experiment run:
# python train.py experiment=example
defaults:
  - override /data: smr_datamodule.yaml
  - override /model: TSCeption.yaml
  - override /trainer: gpu.yaml
  - override /paths: default.yaml
  - override /extras: default.yaml
  - override /hydra: default.yaml
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: [ "torcheeg", "TSCeption", "Within-Subject", "RL" ]

seed: 12345

data:
  task_name: "LR"
  ival: 1s:10s:1ms
  chans: [ "*" ]
  bands:
    - 8
    - 40
  loading_data_mode: "cross_subject_hpo"
  transform: "TSCeption"
  normalize: { norm_type: "std", norm_axis: 0 }
  train_val_split: False
  cross_validation: { num_splits: 3, split_seed: 1234 }
  batch_size: 32
  num_workers: 4
  pin_memory: True

model:
  criterion:
    _target_: torch.nn.BCEWithLogitsLoss
    _partial_: true
  plots_settings:
    plot_every_n_epoch: 10
  net:
    num_classes: 4
    num_electrodes: 54

trainer:
  min_epochs: 30
  max_epochs: 60
  check_val_every_n_epoch: 1

paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: /input-data
  log_dir: ${oc.env:PROJECT_ROOT}/logs
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}

logger:
  mlflow:
    experiment_name: TSception-HPO-LR
    run_name: S1

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}"
    monitor: "val/acc_best"
    mode: "max"
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/acc_best"
    patience: 20
    mode: "max"